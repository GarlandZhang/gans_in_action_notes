## Chapter 6. Progressing with GANs

"As our mountaineer gets closer to a valley, we can start increasing the complexity by zooming in on the terrain. Then we no longer see just the coarse/pixelated texture, but instead get to see the finer details. This approach has the advantage that as our mountaineer goes down the slope, they can easily make little optimizations to make the hiking easier. For example, they can take a path through a dried-up creek to make the descent into the valley even faster. That is progressive growing: increasing the resolution of the terrain as we go."

"we progressively smooth in and slowly introduce more complexity as the mountaineer gets closer to the objective"

![progressive_size](https://i.gyazo.com/393bbc384daee809adf819de44456ff3.png)

" rather than immediately jumping to this resolution, we smoothly fade in this new layer with higher resolution by a parameter alpha (α), which is between 0 and 1. Alpha affects how much we use either the old—but upscaled—layer or the natively larger one. On the side of the D, we simply shrink by 0.5x to allow for smoothly injecting the trained layer for discrimination."


### Mini-batch standard deviation

Recall: "the issue of mode collapse, which occurs when the GAN learns how to create a few good examples or only slight permutations on them. We generally want to produce the faces of all the people in the real dataset, maybe not just one picture of one woman."

to tell whether samples are varied enough, "we calculate a single extra scalar statistic for the Discriminator [aka] the standard deviation of all the pixels in the mini-batch that are generated by the Generator or that come from the real data [...]  all the Discriminator needs to learn is that if the standard deviation is low in the images from the batch it is evaluating, the image is likely fake, because the real data has more variance.[2] The Generator has no choice but to increase the variance of the generated samples to have a chance to fool the Discriminator"

"the technical implementation is straightforward as it applies only to the Discriminator. Given that we also want to minimize the number of trainable parameters, we include only a single extra number, which seems to be enough. This number is appended as a feature map—think dimension or the last number in the tf.shape list."

### Pixel wise feature normalization in generator

"Let’s begin with some motivation for why would we even want to normalize the features—stability of training. Empirically, the authors from NVIDIA have discovered that one of the early signs of divergent training was an explosion in feature magnitudes. A similar observation was made by the BigGAN authors in chapter 12. So Karras et al. introduced a technique to combat this. On a broader note, this is frequently how GAN training is done: we observe a particular problem with the training, so we introduce mechanisms to prevent that problem from happening."

![pixel_wise_norm](https://i.gyazo.com/4ea1ac3afacb90fccff4b89ce5349285.png)


