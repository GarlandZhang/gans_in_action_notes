{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "progessive_gan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7b82/Q2s7tnNTwc+m7k8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GarlandZhang/gans_in_action_notes/blob/master/progessive_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv_L-nEQyqlM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34f10e3e-9957-4539-f001-b099a46eda68"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc3Tu5SizzF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upscale_layer(layer, upscale_factor):\n",
        "  height = layer.get_shape()[1]\n",
        "  width = layer.get_shape()[2]\n",
        "  size = (upscale_factor * height, upscale_factor * width)\n",
        "  upscaled_layer = tf.image.resize_nearest_neighbor(layer, size)\n",
        "  return upscaled_layer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSjjOKNE0JWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smoothly_merge_last_layer(list_of_layers, alpha):\n",
        "  last_fully_trained_layer = list_of_layers[-2]\n",
        "  last_layer_upscaled = upscale_layer(last_fully_trained_layer, 2)\n",
        "  larger_native_layer = list_of_layers[-1]\n",
        "  assert larger_native_layer.get_shape() == last_layer_upscaled.get_shape()\n",
        "  new_layer = (1 - alpha) * upscaled_layer + alpha * larger_native_layer # alpha slowly increased as the larger native layer is being improved\n",
        "  return new_layer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4UnvcVu3HXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def minibatch_std_layer(layer, group_size=4): # group size is mini batch size\n",
        "  group_size = K.backend.minimum(group_size, tf.shape(layer)[0])\n",
        "  shape = list(K.int_shape(input))\n",
        "  shape[0] = tf.shape(input)[0]\n",
        "  minibatch = K.backend.reshape(layer, (group_size, -1, shape[1], shape[2], shape[3]))\n",
        "  minibatch -= tf.reduce_mean(minibatch, axis=0, keepdims=True) # center the mean over the group\n",
        "  minibatch = tf.reduce_mean(K.backend.square(minibatch), axis=0) # calculate variance\n",
        "  minibatch = K.backend.square(minibatch + 1e8) # calculate standard dev\n",
        "  minibatch = tf.reduce_mean(minibatch, axis=[1, 2, 4], keepdims=True) # takes average over all pixels\n",
        "  minibatch = K.backend.tile(minibatch, [group_size, 1, shape[2], shape[3]]) # transform scalar value to fit groups and pixels (?)\n",
        "\n",
        "  return K.backend.concatenate([layer, minibatch], axis=1) # appends as new feature map"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p5y1nEzFPEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def equalize_learning_rate(shape, gain, fan_in=None):\n",
        "  '''\n",
        "  This adjusts weights of every layer by the constant from He's initializer so that we adjust for the variance in the dynamic range in different features\n",
        "  shape: shape of tensor (layer) or the dimensions\n",
        "  e.g.; [4, 4, 48, 3] => [kernel_size, kernel_size, number_of_filters, feature_maps]\n",
        "  '''\n",
        "\n",
        "  if fan_in == None:\n",
        "    fan_in = np.prod(shape[:-1]) # \"the product of all the shape dimensions minus the feature maps dimension; this gives us the number of incoming connections per neuron\"\n",
        "\n",
        "  std = gain / K.sqrt(fan_in) # He's constant\n",
        "\n",
        "  wscale = K.constant(std, name='wscale', dtype=np.float32) # \"creates a constant out of the adjustment\"\n",
        "\n",
        "  adjusted_weights = K.get_value('layer', shape=shape, initializer=tf.initializers.random_normal()) * wscale # \"Gets values for weights and then uses broadcasting to apply the adjustment\"\n",
        "  \n",
        "  return adjusted_weights\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq3G2hVDk9Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pixelwise_feat_norm(inputs, **kwargs):\n",
        "  '''\n",
        "  Use pixelwise feature normalization\n",
        "  '''\n",
        "  normalization_constant = K.backend.sqrt(K.backend.mean(inputs**2, axis=-1, keepdims=True) + 1.0e-8)\n",
        "\n",
        "  return inputs / normalization_constant"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3jNV4BWsiPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}