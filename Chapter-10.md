## Chapter 10. Adversarial examples

big problem with GANs is a little bit of noise drastically changes predictions. 

**see adversarial_examples.ipynb for resnet prediction on original vs adversarial**

we discover that it predicts very differently after applying a bit of noise to the image, even though to the human eye they look identical. This is an adversarial example.

"Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to cause the model to make a mistake; they're like optical illusions for machines"
